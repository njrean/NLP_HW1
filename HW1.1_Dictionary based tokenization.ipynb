{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"IRBubL3il3TB"},"source":["### Natcha Jangphiphatnawakit 63340500031"]},{"cell_type":"markdown","metadata":{"id":"7EdbVSmE87En"},"source":["# HW1: Dictionary-based Tokenization \n"]},{"cell_type":"markdown","metadata":{"id":"pJJLm1Ub87Et"},"source":["In this exercise, you are to implement a dictionary-based word segmentation algorithm. There are two Python functions that you need to complete: \n","<br>\n","* maximal_matching\n","* backtrack\n","</br>\n","\n","Also, you have to find how to use word_tokenize() in PythaiNLP along with customer_dict by yourselves."]},{"cell_type":"markdown","metadata":{"id":"DF5Pme7CK3YF"},"source":["## Part1) Your Maximal Matching with Your Dictionary"]},{"cell_type":"markdown","metadata":{"id":"xzs0R06q87Et"},"source":["### Create a toy dictionary to test the algorithm\n","\n","This is based on the example shown in the lecture. \n","You will tokenize the following text string: \"ไปหามเหสี!\"\n","The toy dictoionary provided in this exercise includes all the charaters, syllables, and words that appear that the text string."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"pq3W4p3z87Ev"},"outputs":[],"source":["thai_vocab = [\"ไ\",\"ป\",\"ห\",\"า\",\"ม\",\"เ\",\"ห\",\"ส\",\"ี\",\"ไป\",\"หา\",\"หาม\",\"เห\",\"สี\",\"มเหสี\",\"!\"]"]},{"cell_type":"markdown","metadata":{"id":"ZornooGF87Ew"},"source":["### Maximal matching \n","Complete the maximal matching  function below to tokenize the input text\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Ao4d2E3387Ew"},"outputs":[],"source":["# c is the character array\n","from math import inf #infinity\n","def maximal_matching(c):\n","    #Initialize an empty 2D list\n","    d  =[[None]*len(c) for _ in range(len(c))]\n","    \n","    ####FILL CODE HERE####\n","\n","    collect = []\n","    word_ls = []\n","    check_ls = []\n","    ch = \"\"\n","    flag = 0\n","    add = 0\n","\n","    for i in range(len(c)):\n","\n","        for j in range(len(c)-i):\n","            \n","            ch += c[j+i]\n","           \n","            if ch in thai_vocab:\n","                if (flag == 1):\n","                    check_ls = check_ls[0:-1]\n","\n","                check_ls.append(ch)\n","                word_ls.append(ch)\n","    \n","                d[i][j+i] = len(check_ls) + add\n","\n","                flag = 1\n","             \n","            else:\n","                d[i][j+i] = inf\n","        \n","        flag = 0\n","        collect.append(word_ls) \n","        ch = \"\"\n","        check_ls = []\n","        ls = [list(e) for e in zip(*d)]\n","        add = min(value for value in ls[i] if value is not None)\n","    \n","    ######################\n","    \n","    return d"]},{"cell_type":"markdown","metadata":{"id":"w7vBXfjM87Ew"},"source":["### Backtracking\n","Complete the backtracking function below to find the tokenzied words.\n","It should return a list containing a pair of the beginning position and the ending position of each word.\n","In this example, it should return: \n","<br>\n","[(0, 1),(2, 3),(4, 8),(9, 9)]\n","<br> \n","#### Each pair contains the position of each word as follows:\n","(0, 1) ไป\n","<br>\n","(2, 3) หา\n","<br>\n","(4, 8) มเหสี\n","<br>\n","(9, 9) !\n"]},{"cell_type":"code","execution_count":142,"metadata":{"id":"SxNFf1IE87Ex"},"outputs":[],"source":["def backtrack(d):\n","    eow = len(d)-1 # End of Word position\n","    word_pos = [] # Word position\n","    ####FILL CODE HERE####\n","\n","    d_T = [list(e) for e in zip(*d)]\n","    idx_x = eow\n","\n","    while(idx_x > 0):\n","        pos = []\n","        v = min(value for value in d_T[idx_x]  if value is not None)\n","        idx_y = d_T[idx_x].index(v)\n","        pos.append(idx_x)\n","        \n","        while(d[idx_y][idx_x] != None):\n","            idx_x -=1\n","            if idx_x == 0:\n","                idx_x = -1\n","                break\n","            \n","        pos.append(idx_x+1)\n","        pos.reverse()\n","        word_pos.append(tuple(pos))\n","\n","    ######################\n","    word_pos.reverse()\n","    return word_pos\n"]},{"cell_type":"markdown","metadata":{"id":"q0MJkKsh87Ex"},"source":["### Test your maximal matching algorithm on a toy dictionary\n","\n","Expected output:\n","\n","[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n","<br>\n","[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n","<br>\n","[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n","<br>\n","[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n","<br>\n","[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n","<br>\n","[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n","<br>\n","[None, None, None, None, None, None, 4, inf, inf, inf] ห\n","<br>\n","[None, None, None, None, None, None, None, 4, 4, inf] ส\n","<br>\n","[None, None, None, None, None, None, None, None, 5, inf] ี\n","<br>\n","[None, None, None, None, None, None, None, None, None, 4] !\n","<br>"]},{"cell_type":"code","execution_count":143,"metadata":{"id":"tsmVQIKS87Ey"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n","[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n","[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n","[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n","[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n","[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n","[None, None, None, None, None, None, 4, inf, inf, inf] ห\n","[None, None, None, None, None, None, None, 4, 4, inf] ส\n","[None, None, None, None, None, None, None, None, 5, inf] ี\n","[None, None, None, None, None, None, None, None, None, 4] !\n"]}],"source":["input_text = \"ไปหามเหสี!\"\n","out = maximal_matching(input_text)\n","for i in range(len(out)):\n","    print(out[i],input_text[i])"]},{"cell_type":"markdown","metadata":{"id":"IVhCMM4d87Ey"},"source":["### Test your backtracking algorithm on a toy dictionary\n","Expected output:\n","<br>\n","ไป|หา|มเหสี|!"]},{"cell_type":"code","execution_count":145,"metadata":{"id":"6Hurbm1f87Ey"},"outputs":[{"name":"stdout","output_type":"stream","text":["ไป|หา|มเหสี|!\n"]}],"source":["def print_tokenized_text(d, input_text):\n","    tokenized_text=[]\n","    for pos in backtrack(d):\n","        # print(pos)\n","        tokenized_text.append(input_text[pos[0]:pos[1]+1])\n","\n","    print(\"|\".join(tokenized_text))\n","    \n","print_tokenized_text(out,input_text)"]},{"cell_type":"markdown","metadata":{"id":"57rP9cTU87Ez"},"source":["## Part2) Your Maximal Matching with Real Dictionary"]},{"cell_type":"markdown","metadata":{"id":"V306h7AG87Ez"},"source":["For UNIX-based OS users, the following cell will download a dictionary (it's just a list of thai words). Alternatively, you can download it from this link: https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63,"status":"ok","timestamp":1675821102082,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"EFVR9LO187Ez","outputId":"1f38b1ec-8f67-4afe-d173-921993033fbf","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-02-08 01:51:43--  https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1519221 (1.4M) [text/plain]\n","Saving to: ‘words_th.txt’\n","\n","words_th.txt        100%[===================>]   1.45M  --.-KB/s    in 0.06s   \n","\n","2023-02-08 01:51:43 (23.5 MB/s) - ‘words_th.txt’ saved [1519221/1519221]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"]},{"cell_type":"code","execution_count":147,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1675821102084,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"nqIQzVgE87E0","outputId":"f38bc3d8-83b8-4485-8bc3-af1504b8fdbf"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'words_th.txt'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[147], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mwords_th.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8-sig\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      2\u001b[0m     thai_vocab \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39msplitlines() \n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mVocab size:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(thai_vocab))\n","File \u001b[1;32mc:\\Users\\natch\\.conda\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'words_th.txt'"]}],"source":["with open(\"words_th.txt\",encoding='utf-8-sig') as f:\n","    thai_vocab = f.read().splitlines() \n","print(\"Vocab size:\", len(thai_vocab))\n","thai_vocab.extend([\"ๆ\",\"!\"])"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1675821631374,"user":{"displayName":"Paisit Khanarsa","userId":"13279666281938719332"},"user_tz":-420},"id":"LwtodW_5wDkN","outputId":"1a238d7f-e313-483d-ea2e-93e1b82ed657"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":[" \"ไป\" in thai_vocab"]},{"cell_type":"markdown","metadata":{"id":"Kpjwzw1w87E0"},"source":["### The output of your maximal matching algoithm on a new dictionary\n","Expected output:\n","<br>\n","[1, 1, 100000, 1, 100000, 100000, 100000, 100000, 100000] ไ\n","<br>\n","[None, 2, 100000, 100000, 100000, 100000, 100000, 100000, 100000] ป\n","<br>\n","[None, None, 2, 2, 2, 100000, 100000, 100000, 100000] ห\n","<br>\n","[None, None, None, 100000, 100000, 100000, 100000, 100000, 100000] า\n","<br>\n","[None, None, None, None, 2, 100000, 100000, 100000, 2] ม\n","<br>\n","[None, None, None, None, None, 100000, 3, 100000, 100000] เ\n","<br>\n","[None, None, None, None, None, None, 100001, 100000, 100000] ห\n","<br>\n","[None, None, None, None, None, None, None, 4, 4] ส\n","<br>\n","[None, None, None, None, None, None, None, None, None] ี"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYD5ChIS87E0"},"outputs":[],"source":["input_text = \"ไปหามเหสี\"\n","out = maximal_matching(input_text)\n","for i in range(len(out)):\n","    print(out[i],input_text[i])"]},{"cell_type":"markdown","metadata":{"id":"BSqLuK7G87E0"},"source":["### Expected tokenized text\n","ไปหา|มเหสี"]},{"cell_type":"code","execution_count":146,"metadata":{"id":"TI077jmy87E0"},"outputs":[{"name":"stdout","output_type":"stream","text":["ไป|หา|มเหสี|!\n"]}],"source":["print_tokenized_text(out,input_text)"]},{"cell_type":"markdown","metadata":{"id":"VLGgO8PrLSz6"},"source":["## Part3) Maximal Matching from PythaiNLP"]},{"cell_type":"markdown","metadata":{"id":"LrZrzQoXLeUX"},"source":["### Default dictionary\n","\n","Study word_tokenize() from PythaiNLP in the link below.\n","\n","https://thainlp.org/pythainlp/docs/2.0/api/tokenize.html"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"yXxPBOcNLXfm"},"outputs":[{"name":"stderr","output_type":"stream","text":["'marisa_trie' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["# !pip install pythainlp\n","# !pip install marisa_trie"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"goQE5gFUL4KO"},"outputs":[],"source":["from pythainlp.tokenize import word_tokenize\n","text='นัดกินกันตอนไหนก็ได้ที่สามย่านมิตรทาวน์'\n","\n","####FILL CODE HERE####\n","    \n","######################"]},{"cell_type":"markdown","metadata":{"id":"2SlX5cEBMHPd"},"source":["### Custom dictionary\n","\n","Add 'สามย่านมิตรทาวน์' into dictionary and then tokenize again"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4V9TqFaMPAj"},"outputs":[],"source":["####FILL CODE HERE####\n","    \n","######################"]}],"metadata":{"colab":{"collapsed_sections":["DF5Pme7CK3YF","xzs0R06q87Et","ZornooGF87Ew","w7vBXfjM87Ew","q0MJkKsh87Ex","57rP9cTU87Ez","Kpjwzw1w87E0","BSqLuK7G87E0","VLGgO8PrLSz6","LrZrzQoXLeUX","2SlX5cEBMHPd"],"provenance":[]},"kernelspec":{"display_name":"nlp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"97c267b1f2113ed722464dc9bd49a318e7d979aff4433b0f75162aa4b606adbd"}}},"nbformat":4,"nbformat_minor":0}
