{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenizer exercise#\n",
    "\n",
    "In this exercise, you are going to implement a (sort of) real world task using Tensorflow and Keras. Tensorflow is a deep learning framwork developed by Google and Keras is a frontend library built on top of Tensorflow (and Theano) to provide an easier way to use standard layers and networks.\n",
    "\n",
    "To complete this exercise, you will need to build deep learning models for word tokenization in Thai (แบ่งเว้นวรรคภาษาไทย) using NECTEC's BEST corpus, one model for each of the following type:\n",
    "- Feedforward Neural Network\n",
    "- One-Dimentional Convolution Neural Network (1D-CNN)\n",
    "- Recurrent Neural Network (RNN, LSTM, or GRU)\n",
    "\n",
    "and one more model of your choice to achieve highest score possible.\n",
    "\n",
    "We provide code for data cleaning and starter code for keras in this notebook but feel free to modify those parts to suit your needs. You can also complete this exercise using only Tensorflow (without using Keras) or using additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
    "\n",
    "This notebook assumes you have already installed Tensorflow and Keras with python3 and had GPU enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run setup code\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "# You don't need to run the following code as we already did it for you to give everyone the same dataset\n",
    "# import cattern.data_utils\n",
    "# cattern.data_utils.generate_best_dataset(os.getcwd()+'/data', create_val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we are going to build a word tokenization model which is a binary classification model trying to predict whether a character is the begining of the word or not (if it is, then there is a space in front of it) and without using any knowledge about type of character (vowel, number, English character etc.).\n",
    "\n",
    "For example,\n",
    "\n",
    "'แมวดำน่ารักมาก' -> 'แมว ดำ น่า รัก มาก'\n",
    "\n",
    "will have these true labels:\n",
    "\n",
    "[(แ,1), (ม,0), (ว,0) (ด,1), ( ำ,0), (น,1), (-่,0), (า,0), (ร,1), (-ั,0), (ก,0), (ม,1), (า,0), (ก,0)]\n",
    "\n",
    "In this task, we will use only the character in question and the characters that surround it but you can imagine that a more complex model will try to include knowledge about each character into the model.\n",
    "You can do that too if you feel like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create map of dictionary to character\n",
    "CHARS = [\n",
    "  '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
    "  ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
    "  '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E',\n",
    "  'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n",
    "  'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_',\n",
    "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "  'n', 'o', 'other', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
    "  'z', '}', '~', 'ก', 'ข', 'ฃ', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช',\n",
    "  'ซ', 'ฌ', 'ญ', 'ฎ', 'ฏ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท',\n",
    "  'ธ', 'น', 'บ', 'ป', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ', 'ม', 'ย', 'ร', 'ฤ',\n",
    "  'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ', 'ฯ', 'ะ', 'ั', 'า',\n",
    "  'ำ', 'ิ', 'ี', 'ึ', 'ื', 'ุ', 'ู', 'ฺ', 'เ', 'แ', 'โ', 'ใ', 'ไ',\n",
    "  'ๅ', 'ๆ', '็', '่', '้', '๊', '๋', '์', 'ํ', '๐', '๑', '๒', '๓',\n",
    "  '๔', '๕', '๖', '๗', '๘', '๙', '‘', '’', '\\ufeff'\n",
    "]\n",
    "CHARS_MAP = {v: k for k, v in enumerate(CHARS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_n_gram_df(df, n_pad):\n",
    "  \"\"\"\n",
    "  Given input dataframe, create feature dataframe of shifted characters\n",
    "  Input:\n",
    "  df: timeseries of size (N)\n",
    "  n_pad: a number of context, for a given character at position idx\n",
    "    character at position [idx-n_pad/2 : idx+n_pad/2] will be used \n",
    "    as features for that character.\n",
    "  \n",
    "  Output:\n",
    "  dataframe of size (N * n_pad) which each row contains the character, \n",
    "    n_pad_2 characters to the left, and n_pad_2 characters to the right\n",
    "    of that character.\n",
    "  \"\"\"\n",
    "  n_pad_2 = int((n_pad - 1)/2)\n",
    "  for i in range(n_pad_2):\n",
    "      df['char-{}'.format(i+1)] = df['char'].shift(i + 1)\n",
    "      df['char{}'.format(i+1)] = df['char'].shift(-i - 1)\n",
    "  return df[n_pad_2: -n_pad_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_feature(best_processed_path, option='train'):\n",
    "  \"\"\"\n",
    "  Transform processed path into feature matrix and output array\n",
    "  Input:\n",
    "  best_processed_path: str, path to processed BEST dataset\n",
    "  option: str, 'train' or 'test'\n",
    "  \"\"\"\n",
    "  # padding for training and testing set\n",
    "  n_pad = 21\n",
    "  n_pad_2 = int((n_pad - 1)/2)\n",
    "  pad = [{'char': ' ', 'target': True}]\n",
    "  df_pad = pd.DataFrame(pad * n_pad_2)\n",
    "\n",
    "  df = []\n",
    "  # article types in BEST corpus\n",
    "  article_types = ['article', 'encyclopedia', 'news', 'novel']\n",
    "  for article_type in article_types:\n",
    "      df.append(pd.read_csv(os.path.join(best_processed_path, option, 'df_best_{}_{}.csv'.format(article_type, option))))\n",
    "  \n",
    "  df = pd.concat(df)\n",
    "  # pad with empty string feature\n",
    "  df = pd.concat((df_pad, df, df_pad))\n",
    "\n",
    "  # map characters to numbers, use 'other' if not in the predefined character set.\n",
    "  df['char'] = df['char'].map(lambda x: CHARS_MAP.get(x, 80))\n",
    "\n",
    "  # Use nearby character as features\n",
    "  df_with_context = create_n_gram_df(df, n_pad=n_pad)\n",
    "\n",
    "  char_row = ['char' + str(i + 1) for i in range(n_pad_2)] + \\\n",
    "             ['char-' + str(i + 1) for i in range(n_pad_2)] + ['char']\n",
    "\n",
    "  # convert pandas dataframe to numpy array to feed to the model\n",
    "  x_char = df_with_context[char_row].as_matrix()\n",
    "  y = df_with_context['target'].astype(int).as_matrix()\n",
    "\n",
    "  return x_char, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the following commands, we must inform you that our data is quite large and by loading the whole dataset at once will use a lot of memory (~6 GB after processing and up to ~12GB while processing). We expect you to be running this on Google Cloud so that you will not run into this problem. But, if, for any reason, you have to run this on your PC or machine with not enough memory, you might need to write a data generator to process a few entries at a time then feed it to the model while training.\n",
    "\n",
    "For keras, you can use [fit_generator](https://keras.io/getting-started/faq/#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory) to cope with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to the preprocessed data\n",
    "best_processed_path = 'cleaned_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load preprocessed BEST corpus\n",
    "x_train_char, y_train = prepare_feature(best_processed_path, option='train')\n",
    "x_val_char, y_val = prepare_feature(best_processed_path, option='val')\n",
    "x_test_char, y_test = prepare_feature(best_processed_path, option='test')\n",
    "\n",
    "# As a sanity check, we print out the size of the training, val, and test data.\n",
    "print('Training data shape: ', x_train_char.shape)\n",
    "print('Training data labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', x_val_char.shape)\n",
    "print('Validation data labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', x_test_char.shape)\n",
    "print('Test data labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 features:  [[ 112.  140.  114.  148.  130.  142.   94.  142.  128.  128.    1.    1.\n",
      "     1.    1.    1.    1.    1.    1.    1.    1.   97.]\n",
      " [ 140.  114.  148.  130.  142.   94.  142.  128.  128.  141.   97.    1.\n",
      "     1.    1.    1.    1.    1.    1.    1.    1.  112.]\n",
      " [ 114.  148.  130.  142.   94.  142.  128.  128.  141.  109.  112.   97.\n",
      "     1.    1.    1.    1.    1.    1.    1.    1.  140.]]\n",
      "First 30 class labels [1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Print some entry from the data to make sure it is the same as what you think.\n",
    "print('First 3 features: ', x_train_char[:3])\n",
    "print('First 30 class labels', y_train[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, \\\n",
    "    Concatenate, Flatten, SpatialDropout1D, \\\n",
    "    BatchNormalization, Conv1D, Maximum, ZeroPadding1D\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def get_convo_nn2(no_word=101, n_gram=21, no_char=178): # no_word=200\n",
    "    input1 = Input(shape=(n_gram,))\n",
    "    # input2 = Input(shape=(n_gram,))\n",
    "\n",
    "    a = Embedding(no_char, 32, input_length=n_gram)(input1)\n",
    "    a = SpatialDropout1D(0.2)(a)\n",
    "    '''\n",
    "    a2 = Conv1D(no_word, 2, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    a2 = TimeDistributed(Dense(5, input_shape=(n_gram-1, no_word)))(a2)\n",
    "    a2 = ZeroPadding1D(padding=(0, 1))(a2)\n",
    "\n",
    "    a3 = Conv1D(no_word, 3, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    a3 = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(a3)\n",
    "    a3 = ZeroPadding1D(padding=(0, 2))(a3)\n",
    "\n",
    "    a4 = Conv1D(no_word, 4, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    a4 = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(a4)\n",
    "    a4 = ZeroPadding1D(padding=(0, 3))(a4)\n",
    "\n",
    "    a5 = Conv1D(no_word, 5, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    a5 = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(a5)\n",
    "    a5 = ZeroPadding1D(padding=(0, 4))(a5)\n",
    "\n",
    "    a6 = Conv1D(no_word, 6, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    a6 = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(a6)\n",
    "    a6 = ZeroPadding1D(padding=(0, 5))(a6)\n",
    "\n",
    "    a7 = Conv1D(no_word, 7, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    a7 = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(a7)\n",
    "    a7 = ZeroPadding1D(padding=(0, 6))(a7)\n",
    "\n",
    "    a8 = Conv1D(no_word, 8, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    a8 = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(a8)\n",
    "    a8 = ZeroPadding1D(padding=(0, 7))(a8)\n",
    "\n",
    "    a9 = Conv1D(no_word - 50, 9, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    a9 = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(a9)\n",
    "    a9 = ZeroPadding1D(padding=(0, 8))(a9)\n",
    "\n",
    "    a10 = Conv1D(no_word - 50, 10, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    a10 = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(a10)\n",
    "    a10 = ZeroPadding1D(padding=(0, 9))(a10)\n",
    "\n",
    "    a11 = Conv1D(no_word - 50, 11, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    a11 = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(a11)\n",
    "    a11 = ZeroPadding1D(padding=(0, 10))(a11)\n",
    "\n",
    "    a12 = Conv1D(no_word - 100, 12, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    a12 = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(a12)\n",
    "    a12 = ZeroPadding1D(padding=(0, 11))(a12)\n",
    "\n",
    "    a_concat = [a2, a3, a4, a5,\n",
    "                a6, a7, a8, a9,\n",
    "                a10, a11, a12]\n",
    "    a_sum = Maximum()(a_concat)\n",
    "    \n",
    "    b = Embedding(12, 12, input_length=n_gram)(input2)\n",
    "    b = SpatialDropout1D(0.2)(b)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([a, a_sum])\n",
    "    '''\n",
    "    x = BatchNormalization()(a)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x) # new \n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "train with 10 epochs and 256 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/10\n",
      "16461056/16461637 [============================>.] - ETA: 0s - loss: 0.0750 - acc: 0.9718Epoch 00000: val_loss improved from inf to 0.05520, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 893s - loss: 0.0750 - acc: 0.9718 - val_loss: 0.0552 - val_acc: 0.9803\n",
      "Epoch 2/10\n",
      "16461568/16461637 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9789Epoch 00001: val_loss improved from 0.05520 to 0.05097, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 883s - loss: 0.0580 - acc: 0.9789 - val_loss: 0.0510 - val_acc: 0.9822\n",
      "Epoch 3/10\n",
      "16461056/16461637 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9804Epoch 00002: val_loss improved from 0.05097 to 0.04816, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 883s - loss: 0.0542 - acc: 0.9804 - val_loss: 0.0482 - val_acc: 0.9833\n",
      "Epoch 4/10\n",
      "16460800/16461637 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9813Epoch 00003: val_loss improved from 0.04816 to 0.04663, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 883s - loss: 0.0521 - acc: 0.9813 - val_loss: 0.0466 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      "16461056/16461637 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9818Epoch 00004: val_loss improved from 0.04663 to 0.04607, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 879s - loss: 0.0507 - acc: 0.9818 - val_loss: 0.0461 - val_acc: 0.9841\n",
      "Epoch 6/10\n",
      "16460800/16461637 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9822Epoch 00005: val_loss improved from 0.04607 to 0.04543, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 880s - loss: 0.0498 - acc: 0.9822 - val_loss: 0.0454 - val_acc: 0.9844\n",
      "Epoch 7/10\n",
      "16461056/16461637 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9825Epoch 00006: val_loss improved from 0.04543 to 0.04504, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 879s - loss: 0.0491 - acc: 0.9825 - val_loss: 0.0450 - val_acc: 0.9844\n",
      "Epoch 8/10\n",
      "16460800/16461637 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9827Epoch 00007: val_loss improved from 0.04504 to 0.04463, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 878s - loss: 0.0485 - acc: 0.9827 - val_loss: 0.0446 - val_acc: 0.9847\n",
      "Epoch 9/10\n",
      "16461056/16461637 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9829Epoch 00008: val_loss improved from 0.04463 to 0.04448, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 878s - loss: 0.0480 - acc: 0.9829 - val_loss: 0.0445 - val_acc: 0.9848\n",
      "Epoch 10/10\n",
      "16460800/16461637 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9830Epoch 00009: val_loss improved from 0.04448 to 0.04441, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 880s - loss: 0.0477 - acc: 0.9830 - val_loss: 0.0444 - val_acc: 0.9847\n",
      "train with 3 epochs and 512 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/3\n",
      "16460288/16461637 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9836Epoch 00000: val_loss improved from 0.04441 to 0.04305, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 673s - loss: 0.0461 - acc: 0.9836 - val_loss: 0.0430 - val_acc: 0.9854\n",
      "Epoch 2/3\n",
      "16460288/16461637 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9837Epoch 00001: val_loss improved from 0.04305 to 0.04286, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 672s - loss: 0.0458 - acc: 0.9837 - val_loss: 0.0429 - val_acc: 0.9854\n",
      "Epoch 3/3\n",
      "16460288/16461637 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9838Epoch 00002: val_loss improved from 0.04286 to 0.04252, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 672s - loss: 0.0456 - acc: 0.9838 - val_loss: 0.0425 - val_acc: 0.9855\n",
      "train with 3 epochs and 2048 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/3\n",
      "16459776/16461637 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9844Epoch 00000: val_loss improved from 0.04252 to 0.04177, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 414s - loss: 0.0440 - acc: 0.9844 - val_loss: 0.0418 - val_acc: 0.9858\n",
      "Epoch 2/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9845Epoch 00001: val_loss improved from 0.04177 to 0.04155, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 414s - loss: 0.0437 - acc: 0.9845 - val_loss: 0.0416 - val_acc: 0.9858\n",
      "Epoch 3/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9845Epoch 00002: val_loss did not improve\n",
      "16461637/16461637 [==============================] - 414s - loss: 0.0435 - acc: 0.9845 - val_loss: 0.0418 - val_acc: 0.9858\n",
      "train with 3 epochs and 4096 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9848Epoch 00000: val_loss improved from 0.04155 to 0.04104, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 387s - loss: 0.0430 - acc: 0.9848 - val_loss: 0.0410 - val_acc: 0.9860\n",
      "Epoch 2/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9848Epoch 00001: val_loss did not improve\n",
      "16461637/16461637 [==============================] - 388s - loss: 0.0429 - acc: 0.9848 - val_loss: 0.0412 - val_acc: 0.9861\n",
      "Epoch 3/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9848Epoch 00002: val_loss improved from 0.04104 to 0.04084, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 388s - loss: 0.0429 - acc: 0.9848 - val_loss: 0.0408 - val_acc: 0.9861\n",
      "train with 3 epochs and 8192 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9849Epoch 00000: val_loss improved from 0.04084 to 0.04077, saving model to weight/model_weight_1.h5\n",
      "16461637/16461637 [==============================] - 374s - loss: 0.0424 - acc: 0.9849 - val_loss: 0.0408 - val_acc: 0.9862\n",
      "Epoch 2/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9849Epoch 00001: val_loss did not improve\n",
      "16461637/16461637 [==============================] - 374s - loss: 0.0425 - acc: 0.9849 - val_loss: 0.0411 - val_acc: 0.9861\n",
      "Epoch 3/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9850Epoch 00002: val_loss did not improve\n",
      "16461637/16461637 [==============================] - 373s - loss: 0.0424 - acc: 0.9850 - val_loss: 0.0408 - val_acc: 0.9861\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "weight_path='weight/model_weight_1.h5'\n",
    "callbacks_list = [\n",
    "        ReduceLROnPlateau(),\n",
    "        ModelCheckpoint(\n",
    "            weight_path,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "  ]\n",
    "\n",
    "verbose = 1\n",
    "# verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = verbose, 2 = one log line per epoch.\n",
    "print('start training')\n",
    "# train model\n",
    "model = get_convo_nn2()\n",
    "train_params = [(10, 256), (3, 512), (3, 2048), (3, 4096), (3, 8192)]\n",
    "for (epochs, batch_size) in train_params:\n",
    "  print(\"train with {} epochs and {} batch size\".format(epochs, batch_size))\n",
    "  if validation_set:\n",
    "    model.fit(x_train_char, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "              callbacks=callbacks_list,\n",
    "              validation_data=(x_val_char, y_val))\n",
    "  else:\n",
    "    model.fit(x_train_char, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "              callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with 3 epochs and 2048 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/3\n",
      "16461637/16461637 [==============================] - 391s - loss: 0.0522 - acc: 0.9812 - val_loss: 0.0468 - val_acc: 0.9835\n",
      "Epoch 2/3\n",
      "16461637/16461637 [==============================] - 391s - loss: 0.0519 - acc: 0.9813 - val_loss: 0.0468 - val_acc: 0.9835\n",
      "Epoch 3/3\n",
      "16461637/16461637 [==============================] - 391s - loss: 0.0517 - acc: 0.9814 - val_loss: 0.0465 - val_acc: 0.9836\n",
      "train with 3 epochs and 4096 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/3\n",
      "16461637/16461637 [==============================] - 365s - loss: 0.0511 - acc: 0.9816 - val_loss: 0.0460 - val_acc: 0.9838\n",
      "Epoch 2/3\n",
      "16461637/16461637 [==============================] - 366s - loss: 0.0510 - acc: 0.9817 - val_loss: 0.0458 - val_acc: 0.9838\n",
      "Epoch 3/3\n",
      "16461637/16461637 [==============================] - 365s - loss: 0.0510 - acc: 0.9816 - val_loss: 0.0460 - val_acc: 0.9837\n",
      "train with 3 epochs and 8192 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/3\n",
      "16461637/16461637 [==============================] - 353s - loss: 0.0504 - acc: 0.9818 - val_loss: 0.0456 - val_acc: 0.9840\n",
      "Epoch 2/3\n",
      "16461637/16461637 [==============================] - 353s - loss: 0.0505 - acc: 0.9819 - val_loss: 0.0456 - val_acc: 0.9839\n",
      "Epoch 3/3\n",
      "16461637/16461637 [==============================] - 352s - loss: 0.0503 - acc: 0.9819 - val_loss: 0.0456 - val_acc: 0.9839\n"
     ]
    }
   ],
   "source": [
    "train_params = [(3, 2048), (3, 4096), (3, 8192)]\n",
    "for (epochs, batch_size) in train_params:\n",
    "  print(\"train with {} epochs and {} batch size\".format(epochs, batch_size))\n",
    "  if validation_set:\n",
    "    model.fit(x_train_char, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "              callbacks=callbacks_list,\n",
    "              validation_data=(x_val_char, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test_char, y_test = prepare_feature(best_processed_path, option='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "def evaluate(model):\n",
    "  \"\"\"\n",
    "  Evaluate model on splitted 10 percent testing set\n",
    "  \"\"\"\n",
    "\n",
    "  y_predict = model.predict(x_test_char)\n",
    "  y_predict = (y_predict.ravel() > 0.5).astype(int)\n",
    "\n",
    "  f1score = f1_score(y_test, y_predict)\n",
    "  precision = precision_score(y_test, y_predict)\n",
    "  recall = recall_score(y_test, y_predict)\n",
    "\n",
    "  return f1score, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.97495518445136331, 0.97053699312028929, 0.97941378574982818)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(best_processed_path, model) #old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.97809158644993555, 0.97241148881158623, 0.98383843170706664)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(best_processed_path, model) # 2 Layer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_convo5_nn(no_word=100, n_gram=21, no_char=178): # no_word=200\n",
    "    input1 = Input(shape=(n_gram,))\n",
    "    # input2 = Input(shape=(n_gram,))\n",
    "\n",
    "    # a = Embedding(no_char, 32, input_length=n_gram)(input1)\n",
    "    # a = SpatialDropout1D(0.2)(a)\n",
    "    a = input1\n",
    "    \n",
    "    #a5 = Conv1D(no_word, 5, strides=1, padding=\"valid\", activation='relu')(a)\n",
    "    #a5 = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(a5)\n",
    "    #a5 = ZeroPadding1D(padding=(0, 4))(a5)\n",
    "    \n",
    "    #x = Concatenate(axis=-1)([a, a5])\n",
    "    #x = BatchNormalization()(a)\n",
    "\n",
    "    #x = Flatten()(a)\n",
    "    x = Dense(100, activation='relu')(a)\n",
    "    #x = BatchNormalization()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with 10 epochs and 512 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/10\n",
      "16459264/16461637 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8512Epoch 00000: val_loss improved from inf to 0.29250, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 325s - loss: 0.3435 - acc: 0.8512 - val_loss: 0.2925 - val_acc: 0.8739\n",
      "Epoch 2/10\n",
      "16459264/16461637 [============================>.] - ETA: 0s - loss: 0.2641 - acc: 0.8889Epoch 00001: val_loss improved from 0.29250 to 0.24946, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 331s - loss: 0.2641 - acc: 0.8889 - val_loss: 0.2495 - val_acc: 0.8954\n",
      "Epoch 3/10\n",
      "16459776/16461637 [============================>.] - ETA: 0s - loss: 0.2412 - acc: 0.9000Epoch 00002: val_loss improved from 0.24946 to 0.23456, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 324s - loss: 0.2412 - acc: 0.9000 - val_loss: 0.2346 - val_acc: 0.9033\n",
      "Epoch 4/10\n",
      "16460288/16461637 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9060Epoch 00003: val_loss improved from 0.23456 to 0.22870, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 322s - loss: 0.2287 - acc: 0.9060 - val_loss: 0.2287 - val_acc: 0.9072\n",
      "Epoch 5/10\n",
      "16461312/16461637 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9098Epoch 00004: val_loss improved from 0.22870 to 0.21688, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 318s - loss: 0.2208 - acc: 0.9098 - val_loss: 0.2169 - val_acc: 0.9115\n",
      "Epoch 6/10\n",
      "16459776/16461637 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9124Epoch 00005: val_loss did not improve\n",
      "16461637/16461637 [==============================] - 316s - loss: 0.2150 - acc: 0.9124 - val_loss: 0.2235 - val_acc: 0.9071\n",
      "Epoch 7/10\n",
      "16459264/16461637 [============================>.] - ETA: 0s - loss: 0.2104 - acc: 0.9146Epoch 00006: val_loss improved from 0.21688 to 0.20759, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 312s - loss: 0.2104 - acc: 0.9146 - val_loss: 0.2076 - val_acc: 0.9151\n",
      "Epoch 8/10\n",
      "16460800/16461637 [============================>.] - ETA: 0s - loss: 0.2066 - acc: 0.9163Epoch 00007: val_loss improved from 0.20759 to 0.20583, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 308s - loss: 0.2066 - acc: 0.9163 - val_loss: 0.2058 - val_acc: 0.9174\n",
      "Epoch 9/10\n",
      "16459776/16461637 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9181Epoch 00008: val_loss did not improve\n",
      "16461637/16461637 [==============================] - 304s - loss: 0.2030 - acc: 0.9181 - val_loss: 0.2186 - val_acc: 0.9115\n",
      "Epoch 10/10\n",
      "16460800/16461637 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.9198Epoch 00009: val_loss improved from 0.20583 to 0.20190, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 301s - loss: 0.1989 - acc: 0.9198 - val_loss: 0.2019 - val_acc: 0.9184\n",
      "train with 3 epochs and 512 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/3\n",
      "16459776/16461637 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9225Epoch 00000: val_loss improved from 0.20190 to 0.19068, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 301s - loss: 0.1926 - acc: 0.9225 - val_loss: 0.1907 - val_acc: 0.9231\n",
      "Epoch 2/3\n",
      "16460800/16461637 [============================>.] - ETA: 0s - loss: 0.1824 - acc: 0.9267Epoch 00001: val_loss did not improve\n",
      "16461637/16461637 [==============================] - 302s - loss: 0.1824 - acc: 0.9267 - val_loss: 0.2198 - val_acc: 0.9102\n",
      "Epoch 3/3\n",
      "16461312/16461637 [============================>.] - ETA: 0s - loss: 0.1748 - acc: 0.9297Epoch 00002: val_loss did not improve\n",
      "16461637/16461637 [==============================] - 303s - loss: 0.1748 - acc: 0.9297 - val_loss: 0.2490 - val_acc: 0.9028\n",
      "train with 3 epochs and 2048 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/3\n",
      "16455680/16461637 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9388Epoch 00000: val_loss improved from 0.19068 to 0.15712, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 110s - loss: 0.1545 - acc: 0.9388 - val_loss: 0.1571 - val_acc: 0.9389\n",
      "Epoch 2/3\n",
      "16455680/16461637 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9393Epoch 00001: val_loss improved from 0.15712 to 0.15701, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 111s - loss: 0.1536 - acc: 0.9393 - val_loss: 0.1570 - val_acc: 0.9377\n",
      "Epoch 3/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.9404Epoch 00002: val_loss did not improve\n",
      "16461637/16461637 [==============================] - 111s - loss: 0.1510 - acc: 0.9404 - val_loss: 0.1668 - val_acc: 0.9332\n",
      "train with 3 epochs and 4096 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9434Epoch 00000: val_loss improved from 0.15701 to 0.14317, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 94s - loss: 0.1442 - acc: 0.9434 - val_loss: 0.1432 - val_acc: 0.9443\n",
      "Epoch 2/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9442Epoch 00001: val_loss improved from 0.14317 to 0.14074, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 94s - loss: 0.1424 - acc: 0.9442 - val_loss: 0.1407 - val_acc: 0.9455\n",
      "Epoch 3/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.9450Epoch 00002: val_loss did not improve\n",
      "16461637/16461637 [==============================] - 94s - loss: 0.1403 - acc: 0.9450 - val_loss: 0.1462 - val_acc: 0.9431\n",
      "train with 3 epochs and 8192 batch size\n",
      "Train on 16461637 samples, validate on 2035694 samples\n",
      "Epoch 1/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9466Epoch 00000: val_loss did not improve\n",
      "16461637/16461637 [==============================] - 85s - loss: 0.1377 - acc: 0.9466 - val_loss: 0.1413 - val_acc: 0.9446\n",
      "Epoch 2/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9477Epoch 00001: val_loss improved from 0.14074 to 0.13732, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 85s - loss: 0.1343 - acc: 0.9477 - val_loss: 0.1373 - val_acc: 0.9468\n",
      "Epoch 3/3\n",
      "16457728/16461637 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9477Epoch 00002: val_loss improved from 0.13732 to 0.13709, saving model to weight/model_weight_conv0.h5\n",
      "16461637/16461637 [==============================] - 85s - loss: 0.1343 - acc: 0.9477 - val_loss: 0.1371 - val_acc: 0.9471\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "weight_path_conv5='weight/model_weight_conv0.h5'\n",
    "callbacks_list_conv5 = [\n",
    "        ReduceLROnPlateau(),\n",
    "        ModelCheckpoint(\n",
    "            weight_path_conv5,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "  ]\n",
    "\n",
    "verbose = 1\n",
    "model_convo5 = get_convo5_nn()\n",
    "train_params = [(10, 512), (3, 512), (3, 2048), (3, 4096), (3, 8192)]\n",
    "for (epochs, batch_size) in train_params:\n",
    "  print(\"train with {} epochs and {} batch size\".format(epochs, batch_size))\n",
    "  #if validation_set:\n",
    "  model_convo5.fit(x_train_char, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "            callbacks=callbacks_list_conv5,\n",
    "            validation_data=(x_val_char, y_val))\n",
    "  #else:\n",
    "  #  model_convo5.fit(x_train_char, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "  #            callbacks=callbacks_list_conv5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9765748053135006, 0.97086056942490806, 0.98235670449968016)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weight_path_conv5='weight/model_weight_1.h5'\n",
    "#model_convo5 = get_convo5_nn()\n",
    "#model_convo5.load_weights(weight_path_conv5)\n",
    "evaluate(model_convo5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.90810695726913582, 0.90661995670143669, 0.90959884368409827)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model_convo5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
