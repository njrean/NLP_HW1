{"cells":[{"cell_type":"markdown","metadata":{"id":"SpCKO19dhHPq"},"source":["# HW 3 - Neural POS Tagger\n","\n","In this exercise, you are going to build a set of deep learning models on part-of-speech (POS) tagging using Tensorflow 2. Tensorflow is a deep learning framwork developed by Google to provide an easier way to use standard layers and networks.\n","\n","To complete this exercise, you will need to build deep learning models for POS tagging in Thai using NECTEC's ORCHID corpus. You will build one model for each of the following type:\n","\n","- Neural POS Tagging with Word Embedding using Fixed / non-Fixed Pretrained weights\n","- Neural POS Tagging with Viterbi / Marginal CRF\n","\n","Pretrained word embeddding are already given for you to use (albeit, a very bad one).\n","\n","We also provide the code for data cleaning, preprocessing and some starter code for tensorflow 2 in this notebook but feel free to modify those parts to suit your needs. Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n","\n","### Don't forget to change hardware accelrator to GPU in runtime on Google Colab ###"]},{"cell_type":"markdown","metadata":{"id":"Qx0YbM_8hHPt"},"source":["## 1. Setup and Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"yF3lPNNkhHPu"},"source":["We use POS data from [ORCHID corpus](https://www.nectec.or.th/corpus/index.php?league=pm), which is a POS corpus for Thai language.\n","A method used to read the corpus into a list of sentences with (word, POS) pairs have been implemented already. The example usage has shown below.\n","We also create a word vector for unknown word by random."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"cMYhnfv8hwTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GS6O5yT6feRd"},"outputs":[],"source":["%tensorflow_version 2.x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58LvFz30zumq"},"outputs":[],"source":["import shutil\n","shutil.copy(\"/content/drive/MyDrive/FRA 501 IntroNLP&DL/Dataset/resources.zip\", \"/content/resources.zip\")\n","!unzip resources.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2a9b92hYTg2"},"outputs":[],"source":["!pip install python-crfsuite\n","!pip install tensorflow-addons\n","!pip install tf2crf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_c7upY0fYsdt"},"outputs":[],"source":["%tensorflow_version 2.x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJpCbSvJhHPv"},"outputs":[],"source":["from data.orchid_corpus import get_sentences\n","import numpy as np\n","import numpy.random\n","import tensorflow as tf\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4SZz56ThHP0","scrolled":false},"outputs":[],"source":["yunk_emb =np.random.randn(32)\n","train_data = get_sentences('train')\n","test_data = get_sentences('test')\n","print(train_data[0])"]},{"cell_type":"code","source":["yunk_emb"],"metadata":{"id":"sQ_Wp-jRiHTE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awxn_GRIhHP3"},"source":["Next, we load pretrained weight embedding using pickle. The pretrained weight is a dictionary which map a word to its embedding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GS3lTZshHP4"},"outputs":[],"source":["import pickle\n","fp = open('basic_ff_embedding.pt', 'rb')\n","embeddings = pickle.load(fp)\n","fp.close()"]},{"cell_type":"code","source":["embeddings"],"metadata":{"id":"33fe8_A5iNkb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CqTNKsChHP7"},"source":["The given code below generates an indexed dataset(each word is represented by a number) for training and testing data. The index 0 is reserved for padding to help with variable length sequence. (Additionally, You can read more about padding here [https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/])"]},{"cell_type":"markdown","metadata":{"id":"IPGUNEZyhHP8"},"source":["## 2. Prepare Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7fMWn8qehHP9"},"outputs":[],"source":["word_to_idx ={}\n","idx_to_word ={}\n","label_to_idx = {}\n","for sentence in train_data:\n","    for word,pos in sentence:\n","        if word not in word_to_idx:\n","            word_to_idx[word] = len(word_to_idx)+1\n","            idx_to_word[word_to_idx[word]] = word\n","        if pos not in label_to_idx:\n","            label_to_idx[pos] = len(label_to_idx)+1\n","word_to_idx['UNK'] = len(word_to_idx)\n","\n","n_classes = len(label_to_idx.keys())+1"]},{"cell_type":"code","source":["n_classes"],"metadata":{"id":"98R9Bf3jmAjh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QgvZ8v_2hHP_"},"source":["This section is tweaked a little from the demo, word2features will return word index instead of features, and sent2labels will return a sequence of word indices in the sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ktf2KkJghHQA"},"outputs":[],"source":["def word2features(sent, i, emb):\n","    word = sent[i][0]\n","    if word in word_to_idx :\n","        return word_to_idx[word]\n","    else :\n","        return word_to_idx['UNK']\n","\n","def sent2features(sent, emb_dict):\n","    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])\n","\n","def sent2labels(sent):\n","    return numpy.asarray([label_to_idx[label] for (word, label) in sent],dtype='int32')\n","\n","def sent2tokens(sent):\n","    return [word for (word, label) in sent]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgBw3I9ShHQD"},"outputs":[],"source":["sent2features(train_data[100], embeddings)"]},{"cell_type":"markdown","metadata":{"id":"3O7oClK-hHQG"},"source":["Next we create train and test dataset, then we use tensorflow 2 to post-pad the sequence to max sequence with 0. Our labels are changed to a one-hot vector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0tJxtPtohHQH"},"outputs":[],"source":["%%time\n","x_train = np.asarray([sent2features(sent, embeddings) for sent in train_data])\n","y_train = [sent2labels(sent) for sent in train_data]\n","x_test = [sent2features(sent, embeddings) for sent in test_data]\n","y_test = [sent2labels(sent) for sent in test_data]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DG1gvJ4mhHQJ"},"outputs":[],"source":["x_train=tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n","y_train=tf.keras.preprocessing.sequence.pad_sequences(y_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n","x_test=tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=102, dtype='int32', padding='post', truncating='pre', value=0.)\n","y_temp =[]\n","for i in range(len(y_train)):\n","    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\n","y_train = np.asarray(y_temp).reshape(-1,102,n_classes)\n","del(y_temp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZU9x6VdehHQM"},"outputs":[],"source":["print(x_train[100],x_train.shape)\n","print(y_train[100][3],y_train.shape)"]},{"cell_type":"markdown","metadata":{"id":"dx_c3-LwhHQP"},"source":["## 3. Evaluate"]},{"cell_type":"markdown","metadata":{"id":"tvCW24orhHQP"},"source":["Our output from tf keras is a distribution of problabilities on all possible label. outputToLabel will return an indices of maximum problability from output sequence.\n","\n","evaluation_report is the same as in the demo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iqgp2gd4hHQQ"},"outputs":[],"source":["def outputToLabel(yt,seq_len):\n","    out = []\n","    for i in range(0,len(yt)):\n","        if(i==seq_len):\n","            break\n","        out.append(np.argmax(yt[i]))\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pzIL_rsAhHQT"},"outputs":[],"source":["import pandas as pd\n","from IPython.display import display\n","\n","def evaluation_report(y_true, y_pred):\n","    # retrieve all tags in y_true\n","    tag_set = set()\n","    for sent in y_true:\n","        for tag in sent:\n","            tag_set.add(tag)\n","    for sent in y_pred:\n","        for tag in sent:\n","            tag_set.add(tag)\n","    tag_list = sorted(list(tag_set))\n","    \n","    # count correct points\n","    tag_info = dict()\n","    for tag in tag_list:\n","        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n","\n","    all_correct = 0\n","    all_count = sum([len(sent) for sent in y_true])\n","    for sent_true, sent_pred in zip(y_true, y_pred):\n","        for tag_true, tag_pred in zip(sent_true, sent_pred):\n","            if tag_true == tag_pred:\n","                tag_info[tag_true]['correct_tagged'] += 1\n","                all_correct += 1\n","            tag_info[tag_true]['y_true'] += 1\n","            tag_info[tag_pred]['y_pred'] += 1\n","    accuracy = (all_correct / all_count) * 100\n","            \n","    # summarize and make evaluation result\n","    eval_list = list()\n","    for tag in tag_list:\n","        eval_result = dict()\n","        eval_result['tag'] = tag\n","        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n","        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n","        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n","        eval_result['precision'] = precision\n","        eval_result['recall'] = recall\n","        eval_result['f_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n","        \n","        eval_list.append(eval_result)\n","\n","    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f_score': ''})\n","    \n","    df = pd.DataFrame.from_dict(eval_list)\n","    df = df[['tag', 'precision', 'recall', 'f_score', 'correct_count']]\n","    display(df)"]},{"cell_type":"markdown","metadata":{"id":"4YG-RiHdhHQV"},"source":["## 4. Train a model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HrkAiMFhHQW"},"outputs":[],"source":["from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{"id":"U75Ivn2vhHQZ"},"source":["The model is this section is separated to two groups\n","\n","- Neural POS Tagger (4.1)\n","- Neural CRF POS Tagger (4.2)"]},{"cell_type":"markdown","metadata":{"id":"CLwL3B7rhHQZ"},"source":["## 4.1 Neural POS Tagger  (Example)"]},{"cell_type":"markdown","metadata":{"id":"pVoB-1XVhHQa"},"source":["We create a simple Neural POS Tagger as an example for you. This model dosen't use any pretrained word embbeding so it need to use Embedding layer to train the word embedding from scratch.\n","\n","Instead of using tensorflow.keras.models.Sequential, we use tensorflow.keras.models.Model. The latter is better as it can have multiple input/output, of which Sequential model could not. Due to this reason, the Model class is widely used for building a complex deep learning model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxBvv9qfhHQb"},"outputs":[],"source":["inputs = Input(shape=(102,), dtype='int32')\n","output = (Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))(inputs)\n","output = Bidirectional(GRU(32, return_sequences=True))(output)\n","output = Dropout(0.2)(output)\n","output = TimeDistributed(Dense(n_classes,activation='softmax'))(output)\n","model = Model(inputs, output)\n","model.compile(optimizer=Adam(lr=0.001),  loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","\n","model.summary()\n","model.fit(x_train,y=y_train, batch_size=64,epochs=10,verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wo9Da8MThHQf"},"outputs":[],"source":["%%time\n","model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yusa214hhHQh"},"outputs":[],"source":["%%time\n","#model.save_weights('/data/my_pos_no_crf.h5')\n","#model.load_weights('/data/my_pos_no_crf.h5')\n","y_pred=model.predict(x_test)\n","ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n","evaluation_report(y_test, ypred)"]},{"cell_type":"markdown","metadata":{"id":"H0NKka14hHQw"},"source":["## 4.2 CRF Viterbi"]},{"cell_type":"markdown","metadata":{"id":"ijd1rwTghHQx"},"source":["Your next task is to incorporate Conditional random fields (CRF) to your model.\n","\n","To use the CRF layer, you need to use an extension repository for tensorflow library, call tf2crf. If you want to see the detailed implementation, you should read the official tensorflow extention of CRF (https://www.tensorflow.org/addons/api_docs/python/tfa/text).\n","\n","tf2crf link :  https://github.com/xuxingya/tf2crf\n","\n","For inference, you should look at crf.py at the method call and view the input/output argmunets. \n","Link : https://github.com/xuxingya/tf2crf/blob/master/tf2crf/crf.py\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KuybajePhHQy"},"source":["### 4.2.1 CRF without pretrained weight\n","### #TODO 1\n","Incoperate CRF layer to your model in 4.1. CRF is quite complex compare to previous example model, so you should train it with more epoch, so it can converge.\n","\n","To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n","\n","Do not forget to save this model weight."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lEV0q1vAhHQy"},"outputs":[],"source":["# INSERT YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"qOnHO-__kj7T"},"source":["\n","### 4.2.2 CRF with pretrained weight\n","\n","### #TODO 2\n","\n","We would like you create a neural CRF POS tagger model  with the pretrained word embedding as an input and the word embedding is trainable (not fixed). To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n","\n","Please note that the given pretrained word embedding only have weights for the vocabuary in BEST corpus.\n","\n","You can read more information about using predtained weight in embedding layer on Keras from the following link:\n","\n","https://keras.io/examples/nlp/pretrained_word_embeddings/\n","\n","Optionally, you can use your own pretrained word embedding.\n","\n","#### Hint: You can get the embedding from get_embeddings function from embeddings/emb_reader.py . \n","\n","(You may want to read about Tensorflow Masking layer and Trainable parameter)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KVBAv3a9kanH"},"outputs":[],"source":["# INSERT YOUR CODE HERE"]},{"cell_type":"markdown","metadata":{"id":"BYX749sbkzA-"},"source":["### #TODO 3\n","Compare the result between all neural tagger models in 4.1 and 4.2.x and provide a convincing reason and example for the result of these models (which model perform better, why?)\n","\n","(If you use your own weight please state so in the answer)"]},{"cell_type":"markdown","metadata":{"id":"QJKtOsoRhHQv"},"source":["<b>Write your answer here :</b>"]},{"cell_type":"code","source":[],"metadata":{"id":"o51SbTVs1U6Y"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["Qx0YbM_8hHPt","IPGUNEZyhHP8","dx_c3-LwhHQP","4YG-RiHdhHQV","CLwL3B7rhHQZ","H0NKka14hHQw"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}